{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\n",
    "y = \\textbf{f}_{H} (X, \\phi )  \\quad X \\in \\mathbb{R}^{HxWxC} \\quad y \\in \\mathbb{R}^{K}\n",
    "$\n",
    "\n",
    ">>$\n",
    "\\textbf{f} : \\text{Classifier}\\\\\n",
    "y : \\text{class scores}\\\\\n",
    "X : \\text{Image data}\\\\\n",
    "\\phi : \\text{Parameter set}\\\\\n",
    "H : \\text{Hyper-parameter set}\n",
    "$\n",
    "\n",
    "> To solve for $\\phi$, define a loss function and minimize it,\n",
    "\n",
    ">$\n",
    "L(\\textbf{f},y') \\qquad y' \\quad \\text{is target}\\\\\n",
    "\\phi = argmin_{\\phi*} \\big(\\sum_{i}L(\\textbf{f}_{i}, y'_{i}) + \\lambda R(\\textbf{f}) \\big)\n",
    "$\n",
    "\n",
    "> Where\n",
    "\n",
    ">>$\n",
    "R \\quad \\text{is regularizer function}\\\\\n",
    "\\textbf{f} \\in \\text{Reproducing Kernel Hilbert space}\n",
    "$\n",
    "\n",
    "> Regularizers does **implicit dimensionality reduction**. It forces the useless parameters to 0. **Useless dimensions** in parameter space leads to **overfitting** which **leads to large difference between emperical and expected loss.**\n",
    "\n",
    ">> **L1 regularizer** does **hard dimensionality reduction**\n",
    "\n",
    ">> **L2 regularizer** does **soft dimensionality reduction**. That is, dimensions may not actually be 0, but close to 0.\n",
    "\n",
    "> Solve for $\\phi$ using first order gradient descent\n",
    "\n",
    ">$\n",
    "\\phi = \\phi + update(\\Theta, \\nabla_{\\phi}L)\n",
    "$\n",
    "\n",
    "> Where,\n",
    "\n",
    ">>$\n",
    "\\Theta \\quad \\text{are optimization parameters}\\\\\n",
    "\\nabla_{\\phi}L \\quad \\text{is the gradient of loss function w.r.t parameters (refer automatic diff notes)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary hinge loss\n",
    "\n",
    ">$\n",
    "L = \\quad \\mid 1 - yy' \\mid_{+} \\quad = \\quad max \\big(0, 1-yy' \\big)\\\\\n",
    "$\n",
    "\n",
    "> Where,\n",
    "\n",
    ">>$ \n",
    "y = f(X ; \\phi) \\in \\mathbb{R}\\\\\n",
    "y' \\in \\{-1,1\\} \\text{ is the target} \\\\\n",
    "$\n",
    "\n",
    "> This function is **convex but not differentiable**\n",
    "\n",
    ">$\n",
    "yy' \\text{ is positive when prediction and target have same sign and the prediction is greater by a margin of 1}\n",
    "$\n",
    "\n",
    ">> $\n",
    "y = 10 \\quad y' = 1 \\implies yy' = +ve \\quad \\text{and} \\quad L=0\\\\\n",
    "y = -1012 \\quad y' = -1 \\implies yy' = +ve \\quad \\text{and} \\quad L=0\\\\\n",
    "y = 0.1 \\quad y' = 1 \\implies yy' = +ve \\quad \\text{but because of margin} \\quad L=0.9\\\\\n",
    "$\n",
    "\n",
    "> In case of **correct prediction**, the **loss is always 0 no matter how big the classifier output is**.\n",
    "\n",
    "<img src=\"files/hinge.png\" width=500px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi class hinge loss\n",
    "\n",
    ">$\n",
    "L = \\quad \\sum_{j \\neq l} \\mid 1 - (y_{l} - y_{j}) \\mid_{+} \\quad = \\quad max \\big(0, y_{j}-y_{l}+1 \\big)\\\\\n",
    "$\n",
    "\n",
    "> Where,\n",
    "\n",
    ">>$ \n",
    "\\textbf{y} = f(X ; \\phi) \\in \\mathbb{R}^{K}\\\\\n",
    "K \\quad \\text{is the number of classes} \\\\\n",
    "l  \\in \\{0, ... , K \\} \\quad \\text{ is the target label}\n",
    "$\n",
    "\n",
    "> If the score corresponding to target label is higher than scores of all other classes, then loss is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary cross entropy loss\n",
    "\n",
    ">$\n",
    "P(y'=1 \\mid y) = \\sigma(y) = \\frac{1}{1+e^{-y}}\\\\\n",
    "P(y'=-1 \\mid y) = 1-\\sigma(y) = \\frac{1}{1+e^{y}}\\\\\n",
    "P(y' \\mid y) = \\sigma(y)^{y'}\\big(1-\\sigma(y)\\big)^{(1-y')}\n",
    "$\n",
    "\n",
    "> To find the optimum parameters, we minimize the **negative log likelyhood**\n",
    "\n",
    ">$\n",
    "NLL = -\\sum_{i=1}^{N}logP(y'_{i} \\mid y_{i})\n",
    "$\n",
    "\n",
    ">Therefore, loss for each sample is\n",
    "\n",
    ">$\n",
    "L = -logP(y'_{i} \\mid y_{i}) = -(y'log(\\sigma(y)) + (1-y')log(1-\\sigma(y))\n",
    "$\n",
    "\n",
    "> This loss function is **convex and differentiable**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi class softmax loss\n",
    "\n",
    ">$\n",
    "P(y'=l \\mid y) = \\frac{e^{y_l}}{\\sum_{j}e^{y_j}}\n",
    "$\n",
    "\n",
    "> Minimizing **negative log likelihood**, we get the loss for each sample as\n",
    "\n",
    ">$\n",
    "L = -log(\\frac{e^{y_l}}{\\sum_{j}e^{y_j}})\n",
    "$\n",
    "\n",
    "> Note\n",
    "\n",
    ">>$\n",
    "-log(1) = 0\\\\\n",
    "-log(0.001) = 3\\\\\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet loss\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier should be robust to\n",
    "\n",
    "    Translation and rotations\n",
    "    Different lighting condition\n",
    "    Deformation (different poses or body expressions)\n",
    "    Occlusion (when object is hid by another object)\n",
    "    Background clutter (when objects looks blended with background)\n",
    "    \n",
    "Training dataset should contain different variants of same object to make our classifier invariant to those properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classifier : (K) Nearest neighbour\n",
    "\n",
    "> $ y= \\textbf{f}_{K}(x)$ (Non parametric model)\n",
    "\n",
    "> **Training** : Store all image features and its corresponding lables\n",
    "\n",
    "> **Testing** : Find the (top-K) feature(s) in the training set that is(are) closest (according to some distance metric) to the test feature and assign the corresponding label \n",
    "\n",
    "> **Problem**: Test time complexity **O(N)** where N is size of training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear classifier\n",
    "\n",
    ">  $ y= \\textbf{f}_{\\gamma}(x, \\{\\textbf{W},b\\}) = \\textbf{W}x + b$\n",
    "\n",
    ">> $ \\gamma : \\text{Learning rate}\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing hyper parameter\n",
    "\n",
    "> Choose the hyperparameter settings that **work best on both test and validation set**. This is the number that should go into the report\n",
    "\n",
    "> Incase of small datasets, use **cross validation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need both validation and test set?\n",
    "\n",
    "> Gives more confidence on our chosen hyper parameter settings.\n",
    "\n",
    "> From coding point of view, our algorithm should not have access to the labels in the test set. Where as, it can access validation set labels to monitor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset assumptions and traps\n",
    "\n",
    "> It is assumed that all data in datasets are independent and are sampled from same probability distribution. If you happen to collect data over time and assign the test set with data sampled towards the end, then your test set might not actually reapresent the real situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
