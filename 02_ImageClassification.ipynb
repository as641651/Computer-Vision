{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\n",
    "l = \\textbf{f}_{H} (X, \\phi )  \\quad X \\in \\mathbb{R}^{HxWxC} \\quad l \\in \\mathbb{R}^{L}\n",
    "$\n",
    "\n",
    ">>$\n",
    "\\textbf{f} : \\text{Classifier}\\\\\n",
    "l : \\text{class scores}\\\\\n",
    "X : \\text{Image data}\\\\\n",
    "\\phi : \\text{Parameter set}\\\\\n",
    "H : \\text{Hyper-parameter set}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier should be robust to\n",
    "\n",
    "    Translation and rotations\n",
    "    Different lighting condition\n",
    "    Deformation (different poses or body expressions)\n",
    "    Occlusion (when object is hid by another object)\n",
    "    Background clutter (when objects looks blended with background)\n",
    "    \n",
    "Training dataset should contain different variants of same object to make our classifier invariant to those properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classifier : (K) Nearest neighbour\n",
    "\n",
    "> $ l= \\textbf{f}_{K}(x)$ (Non parametric model)\n",
    "\n",
    "> **Training** : Store all image features and its corresponding lables\n",
    "\n",
    "> **Testing** : Find the (top-K) feature(s) in the training set that is(are) closest (according to some distance metric) to the test feature and assign the corresponding label \n",
    "\n",
    "> **Problem**: Test time complexity **O(N)** where N is size of training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear classifier\n",
    "\n",
    ">  $ l= \\textbf{f}_{\\gamma}(x, \\{\\textbf{W},b\\}) = \\textbf{W}x + b$\n",
    "\n",
    ">> $ \\gamma : \\text{Learning rate}\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing hyper parameter\n",
    "\n",
    "> Choose the hyperparameter settings that **work best on both test and validation set**. This is the number that should go into the report\n",
    "\n",
    "> Incase of small datasets, use **cross validation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need both validation and test set?\n",
    "\n",
    "> Gives more confidence on our chosen hyper parameter settings.\n",
    "\n",
    "> From coding point of view, our algorithm should not have access to the labels in the test set. Where as, it can access validation set labels to monitor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset assumptions and traps\n",
    "\n",
    "> It is assumed that all data in datasets are independent and are sampled from same probability distribution. If you happen to collect data over time and assign the test set with data sampled towards the end, then your test set might not actually reapresent the real situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
